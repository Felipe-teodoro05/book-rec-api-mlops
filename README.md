# BookMind: MLOps Recommendation Engine

[![FastAPI](https://img.shields.io/badge/API-FastAPI-009688?style=flat&logo=fastapi&logoColor=white)](https://fastapi.tiangolo.com/)
[![Docker](https://img.shields.io/badge/Deploy-Docker-2496ED?style=flat&logo=docker&logoColor=white)](https://www.docker.com/)
[![PostgreSQL](https://img.shields.io/badge/DB-Neon%20Serverless-336791?style=flat&logo=postgresql&logoColor=white)](https://neon.tech/)
[![Python](https://img.shields.io/badge/Core-Python%203.11-3776AB?style=flat&logo=python&logoColor=white)]()

## Project Overview

O **BookMind** √© um microservi√ßo de alta performance projetado para entregar recomenda√ß√µes personalizadas de livros em tempo real. A arquitetura foi constru√≠da seguindo princ√≠pios de **ML Engineering**, garantindo escalabilidade via Docker e persist√™ncia de dados em nuvem (Neon Serverless Postgres).

Ao contr√°rio de scripts simples de ML, este projeto foca no ciclo completo de vida do modelo (MLOps): desde a engenharia de dados (ETL) at√© a exposi√ß√£o da infer√™ncia via API RESTful.

---

## Architecture & MLOps Strategy

O sistema adota uma estrat√©gia h√≠brida de **Offline Training** com **Online Inference** para garantir baixa lat√™ncia.

```mermaid
graph LR
    subgraph "Data Engineering Layer"
        A["Raw Dataset"] -->|ETL Script| B[("Neon Postgres Cloud")]
    end

    subgraph "Training Pipeline (Offline)"
        B -->|Fetch Data| C["Training Script"]
        C -->|SVD Algorithm| D["Model Artifacts (.pkl)"]
    end

    subgraph "Serving Layer (Online)"
        D -->|Load on Startup| E["FastAPI Container"]
        U["User Request"] -->|GET /recommend| E
        E -->|JSON Response| U
    end
```

### Decis√µes T√©cnicas de Design

- **Modelo SVD (Singular Value Decomposition):** escolhido pela efici√™ncia em filtragem colaborativa. O treinamento √© desacoplado da API, gerando artefatos serializados para infer√™ncia r√°pida.
- **Database Serverless (Neon):** desacoplamento da camada de dados. Permite que a API em Docker seja reiniciada sem perda de estado, simulando um ambiente de produ√ß√£o cloud-native.
- **Containeriza√ß√£o:** o ambiente √© padronizado via `docker-compose`, eliminando problemas de ‚Äúworks on my machine‚Äù.

### Tech Stack

| Componente | Tecnologia | Fun√ß√£o |
|---|---|---|
| API Framework | FastAPI | Interface REST ass√≠ncrona de alta velocidade |
| Server | Uvicorn | Servidor ASGI para produ√ß√£o |
| Container | Docker & Compose | Orquestra√ß√£o do ambiente |
| Database | PostgreSQL (Neon) | Armazenamento persistente na nuvem |
| ML Core | Scikit-Learn | Implementa√ß√£o do algoritmo `TruncatedSVD` |

## Installation & Setup

### Pr√©-requisitos
- Docker Desktop instalado.
- Conta no Neon Tech (ou qualquer Postgres externo).

### 1. Clone & Configura√ß√£o

```bash
git clone https://github.com/Felipe-teodoro05/book-rec-api-mlops.git
cd book-rec-api-mlops
```
### 2. Vari√°veis de Ambiente
Crie um arquivo .env na raiz (n√£o versionado) com sua string de conex√£o:

```text
DATABASE_URL="postgres://usuario:senha@host-do-neon/nome-do-banco?sslmode=require"
```
### 3. Pipeline de Dados (ETL & Treino)
Antes de subir a API, execute o pipeline de dados localmente para popular o banco e gerar o modelo:

```bash
# Setup do ambiente virtual
python -m venv venv
source venv/bin/activate  # Windows: .\venv\Scripts\activate
pip install -r requirements.txt

# Passo 1: Carga de Dados (ETL)
python scripts/load_data_to_neon.py

# Passo 2: Treinamento do Modelo
python scripts/train_model.py
Isso criar√° os arquivos .pkl necess√°rios na pasta app/model_artifacts.
```
### 4. Deploy (Docker)
```bash
docker-compose up --build
Aguarde o log: Uvicorn running on http://0.0.0.0:8000

üì° API Usage
Acesse a documenta√ß√£o interativa (Swagger UI) em: http://localhost:8000/docs

Exemplo de recomenda√ß√£o: GET /recommendations/276747

json
[
  {
    "book_title": "The Lovely Bones",
    "author": "Alice Sebold",
    "score": 0.98
  },
  {
    "book_title": "The Da Vinci Code",
    "author": "Dan Brown",
    "score": 0.95
  }
]
```
Credits & Team
Desenvolvido como projeto pr√°tico de Engenharia de IA.

Felipe Teodoro Bandeira ‚Äî ML Engineering & API

Eduardo Galv√£o ‚Äî Data Pipeline

Jo√£o Victor Ferreira ‚Äî Model Tuning
